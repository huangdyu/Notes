{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "#### Definition\n",
    "\n",
    "A sequential decision problem for a fully observable, **stochastic** environment with a Markovian transition model and additive rewards is called a Markov decision process(MDP)\n",
    "\n",
    "It is defined by:\n",
    "\n",
    "* A set of states $s\\in S$\n",
    "\n",
    "* A set of actiom $a\\in A$\n",
    "\n",
    "* A transition function T(s, a, s') (Markovian)\n",
    "\n",
    "* A reward function R(s, a, s')\n",
    "\n",
    "* A start state $s_0$\n",
    "\n",
    "* A terminal state(optional)\n",
    "\n",
    "#### Solution\n",
    "\n",
    "A solution to a MDP is called a policy $\\pi$\n",
    "\n",
    "* $\\pi(s)$ is the action recommended by the policy $\\pi(s)$ for the state s\n",
    "\n",
    "* An optimal policy $\\pi^*$ is a policy that yields the highest expected utility\n",
    "\n",
    "#### MDP search tree \n",
    "\n",
    "Each MDP state projects an **expectimax-like** search tree\n",
    "\n",
    "s: state; a: action;  --> (s, a): Q state, the agent has committed to the action but has not done it yet.\n",
    "\n",
    "--> via T(s, a, s')  --> s' and get R(s, a, s')\n",
    "\n",
    "#### Utility of State Sequencies\n",
    "\n",
    "(The only way) **disctounting**: consider to maximize the sum of rewards and rewards earlier. \n",
    "\n",
    "--> Values of rewards decay exponentially (with a decay factor $0<\\gamma<1$)\n",
    "\n",
    "#### Stationary preferences\n",
    "\n",
    "Assume an agent's preferences between state sequences are stationary.\n",
    "\n",
    "Then if two states begin with the same state r, then the two sequences should be preference-ordered the same way as the sequences without r, which gives: $$[r, s_1, s_2,\\dots] \\succ [r, s'_1, s'_2, \\dots]$$ \n",
    "\n",
    "Stationarity has strong consequences and it turns out there are **just(only)** two coherent ways to assign utilities to sequences:\n",
    "\n",
    "* Additive rewards ($\\gamma = 1$) and Discounted rewards ($0<\\gamma<1$)\n",
    " \n",
    "For the additive undiscounted rewards will be inifinite: $$\\lim_{t\\to \\infty}U[s_o, s_1, s_2, \\dots, s_t] = \\infty$$ \n",
    "\n",
    "But the discounted rewards have upper bound: $$\\lim_{t\\to \\infty}U[s_o, s_1, s_2, \\dots, s_t] = \\sum_{t = 0}^{\\infty}\\gamma^nR(s_n)\\leq \\frac{R_{max}}{1 - \\gamma}$$\n",
    "\n",
    "#### Optimal Quantities\n",
    "\n",
    "* $V^*(s)$ is the value(utility) of a state s: expected utility starting from s and acting optimally\n",
    "\n",
    "* $Q^*(s,a )$ is the utility of a Q state (s,a)\n",
    "\n",
    "* $\\pi^*(s)$ is the optimal policy for state s\n",
    "\n",
    "By bellman equation: $$V^*(s)= \\mathop{max}\\limits_{a}Q^*(s, a)$$ \n",
    "$$Q^*(s,a) = \\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma V^*(s')]$$\n",
    "$$-->V^*(s)= \\mathop{max}\\limits_{a}\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma V^*(s')]$$\n",
    "\n",
    "#### Computation\n",
    "\n",
    "The MDP tree often goes forever and states are repeated\n",
    "\n",
    "--> Do a depth-limited computation, but with increasing depths until change is small --> (it is gurranted by $\\gamma$)\n",
    "\n",
    "**Time-limited value:**\n",
    "\n",
    "Define $V_k(s)$ to be the optimal value of s if the game ends in k more steps\n",
    "\n",
    "**Value Iteration:**\n",
    "\n",
    "$$V_{k+1}^*(s) \\leftarrow \\mathop{max}\\limits_{a}\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma V^(s')]$$\n",
    "\n",
    "**Policy Methods:**\n",
    "\n",
    "* Policy Evaluation\n",
    "\n",
    "    For a fixed policy, the action in each state is given bt the policy, then $$V^{\\pi}(s) = \\sum_{s'}T(s, \\pi(s), s')[R(s, \\pi(s), s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "    There are two options to calculate the V's for a fixed policy $\\pi$: $$V^{\\pi}_{k+1}(s) \\leftarrow \\sum_{s'}T(s, \\pi(s), s')[R(s, \\pi(s), s') + \\gamma V^{\\pi}_k(s')]$$\n",
    "\n",
    "    Just like the value-iteration, and the complexity is $O(S^2)$ per iteration, compared with $O(AS^2)$, the other option is just use the linear solver\n",
    "\n",
    "* Policy Extraction (gets the policy implied by values)\n",
    "\n",
    "    Instead of extracting from $V^*(s)$, just simply extract from: $$\\pi^*(s) = \\mathop{argmax}\\limits_a Q^*(s,a)$$\n",
    "\n",
    "* Policy Iteration\n",
    "\n",
    "    Two step: (1) for fixed current policy $\\pi$, find values with policy evaluation --> (2) or fixed values, get a better policy using policy extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardenyu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52425358d21c1500e9f7e0b0b3202539c28bb8d46208ac7718471269cd3c6480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
