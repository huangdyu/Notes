{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "#### Definition\n",
    "\n",
    "A sequential decision problem for a fully observable, **stochastic** environment with a Markovian transition model and additive rewards is called a Markov decision process(MDP)\n",
    "\n",
    "It is defined by:\n",
    "\n",
    "* A set of states $s\\in S$\n",
    "\n",
    "* A set of actiom $a\\in A$\n",
    "\n",
    "* A transition function T(s, a, s') (Markovian)\n",
    "\n",
    "* A reward function R(s, a, s')\n",
    "\n",
    "* A start state $s_0$\n",
    "\n",
    "* A terminal state(optional)\n",
    "\n",
    "#### Solution\n",
    "\n",
    "A solution to a MDP is called a policy $\\pi$\n",
    "\n",
    "* $\\pi(s)$ is the action recommended by the policy $\\pi(s)$ for the state s\n",
    "\n",
    "* An optimal policy $\\pi^*$ is a policy that yields the highest expected utility\n",
    "\n",
    "#### MDP search tree \n",
    "\n",
    "Each MDP state projects an **expectimax-like** search tree\n",
    "\n",
    "s: state; a: action;  --> (s, a): Q state, the agent has committed to the action but has not done it yet.\n",
    "\n",
    "--> via T(s, a, s')  --> s' and get R(s, a, s')\n",
    "\n",
    "#### Utility of State Sequencies\n",
    "\n",
    "(The only way) **disctounting**: consider to maximize the sum of rewards and rewards earlier. \n",
    "\n",
    "--> Values of rewards decay exponentially (with a decay factor $0<\\gamma<1$)\n",
    "\n",
    "#### Stationary preferences\n",
    "\n",
    "Assume an agent's preferences between state sequences are stationary.\n",
    "\n",
    "Then if two states begin with the same state r, then the two sequences should be preference-ordered the same way as the sequences without r, which gives: $$[r, s_1, s_2,\\dots] \\succ [r, s'_1, s'_2, \\dots]$$ \n",
    "\n",
    "Stationarity has strong consequences and it turns out there are **just(only)** two coherent ways to assign utilities to sequences:\n",
    "\n",
    "* Additive rewards ($\\gamma = 1$) and Discounted rewards ($0<\\gamma<1$)\n",
    " \n",
    "For the additive undiscounted rewards will be inifinite: $$\\lim_{t\\to \\infty}U[s_o, s_1, s_2, \\dots, s_t] = \\infty$$ \n",
    "\n",
    "But the discounted rewards have upper bound: $$\\lim_{t\\to \\infty}U[s_o, s_1, s_2, \\dots, s_t] = \\sum_{t = 0}^{\\infty}\\gamma^nR(s_n)\\leq \\frac{R_{max}}{1 - \\gamma}$$\n",
    "\n",
    "#### Optimal Quantities\n",
    "\n",
    "* $V^*(s)$ is the value(utility) of a state s: expected utility starting from s and acting optimally\n",
    "\n",
    "* $Q^*(s,a )$ is the utility of a Q state (s,a)\n",
    "\n",
    "* $\\pi^*(s)$ is the optimal policy for state s\n",
    "\n",
    "By bellman equation: $$V^*(s)= \\mathop{max}\\limits_{a}Q^*(s, a)$$ \n",
    "$$Q^*(s,a) = \\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma V^*(s')]$$\n",
    "$$-->V^*(s)= \\mathop{max}\\limits_{a}\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma V^*(s')]$$\n",
    "\n",
    "#### Computation\n",
    "\n",
    "The MDP tree often goes forever and states are repeated\n",
    "\n",
    "--> Do a depth-limited computation, but with increasing depths until change is small --> (it is gurranted by $\\gamma$)\n",
    "\n",
    "**Time-limited value:**\n",
    "\n",
    "Define $V_k(s)$ to be the optimal value of s if the game ends in k more steps\n",
    "\n",
    "**Value Iteration:**\n",
    "\n",
    "$$V_{k+1}^*(s) \\leftarrow \\mathop{max}\\limits_{a}\\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma V^(s')]$$\n",
    "\n",
    "**Policy Methods:**\n",
    "\n",
    "* Policy Evaluation\n",
    "\n",
    "    For a fixed policy, the action in each state is given bt the policy, then $$V^{\\pi}(s) = \\sum_{s'}T(s, \\pi(s), s')[R(s, \\pi(s), s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "    There are two options to calculate the V's for a fixed policy $\\pi$: $$V^{\\pi}_{k+1}(s) \\leftarrow \\sum_{s'}T(s, \\pi(s), s')[R(s, \\pi(s), s') + \\gamma V^{\\pi}_k(s')]$$\n",
    "\n",
    "    Just like the value-iteration, and the complexity is $O(S^2)$ per iteration, compared with $O(AS^2)$, the other option is just use the linear solver\n",
    "\n",
    "* Policy Extraction (gets the policy implied by values)\n",
    "\n",
    "    Instead of extracting from $V^*(s)$, just simply extract from: $$\\pi^*(s) = \\mathop{argmax}\\limits_a Q^*(s,a)$$\n",
    "\n",
    "* Policy Iteration\n",
    "\n",
    "    Two step: (1) for fixed current policy $\\pi$, find values with policy evaluation --> (2) or fixed values, get a better policy using policy extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "#### Basic\n",
    "\n",
    "**Idea**\n",
    "\n",
    "* Receive feedback in the form of rewards\n",
    "\n",
    "* Agentâ€™s utility is defined by the reward function\n",
    "\n",
    "* Must (learn to) act so as to maximize expected rewards\n",
    "\n",
    "* All learning is based on observed samples of outcomes\n",
    "\n",
    "**Compared to MDP:**\n",
    "\n",
    "* A set of states $s\\in S$\n",
    "\n",
    "* A set of actiom $a\\in A$\n",
    "\n",
    "* A transition function T(s, a, s') (Markovian)\n",
    "\n",
    "* A reward function R(s, a, s')\n",
    "\n",
    "And we still try to look for a $\\pi(s)$, but we don't know T or R\n",
    "\n",
    "#### Model-based learning\n",
    "\n",
    "Since we don't know T and R, we consider to learn an approximate model based on experiences, and then it's just like MDP. We can solve for values as if the T and R were correct\n",
    "\n",
    "**Step1:**\n",
    "\n",
    "* count outcome s' for each s, a\n",
    "\n",
    "* normalize to give an estimate of T^(s, a, s')   --> use frequency to replace the probability\n",
    "\n",
    "* discover each R^(s, a, s') when experience (s, a, s')\n",
    "\n",
    "**Step2:**\n",
    "\n",
    "* solve the learned MDP given bt step 1\n",
    "\n",
    "#### MOdel-free learning\n",
    "\n",
    "compare what you want and what you get\n",
    "\n",
    "##### Passive reinforcement learning\n",
    "\n",
    "**Direct evaluation**\n",
    "\n",
    "* Goal: Compute values for each state under $\\pi$\n",
    "\n",
    "* Idea: Average together observed sample values\n",
    "\n",
    "* Advantage: knowledge about T and R is not required; Eventually compute correct values\n",
    "\n",
    "* Disadvantage: State connections are not considered; Each state must be learned separately; Take a long time to learn\n",
    "\n",
    "* Cannot use policy Evaluation since T and R are unknown\n",
    "\n",
    "* --> Instead, use Sample-Based Policy Evaluation(very space consuming and time consuming, because we cannot rewind to state s after take action): $$V_{k+1}^{\\pi}(s)\\leftarrow \\frac{1}{n}\\sum_{i}sample_i$$\n",
    "where $sample_i = R(s, \\pi(s), s'_i)+\\gamma V_{k}^{\\pi}(s'_i)$\n",
    "\n",
    "##### Active reinforcement learning \n",
    "\n",
    "**Temporal Difference Learning**\n",
    "\n",
    "* Idea: learn from every experience\n",
    "\n",
    "* Temporal difference learning of values: $$sample = R(s, \\pi(s), s') + \\gamma V^{\\pi}(s')$$\n",
    "\n",
    "and then update: $$V^{\\pi}(s) \\leftarrow (1-\\alpha)V^{\\pi}(s)+(\\alpha) sample$$\n",
    "it can be also represented as $$V^{\\pi}(s) \\leftarrow V^{\\pi}(s)+\\alpha (sample - V^{\\pi}) \\leftarrow \\text{teporal difference}$$\n",
    "\n",
    "* The running interpolation update makes recent samples more important\n",
    "\n",
    "* Decreasing learning rate (alpha) can give converging averages\n",
    "\n",
    "**Q-learning**\n",
    "\n",
    "* the Q-learning iteration is similar to value-iteration:$$Q_{k+1}(s, a)\\leftarrow \\sum_{s'}T(s, a, s')[R(s, a, s') + \\gamma \\mathop{max}\\limits_{a'}Q_{k}(s', a')]$$\n",
    "where $\\mathop{max}\\limits_{a'}Q_{k}(s', a') = V_k(s')$\n",
    "\n",
    "* Sample-based Q-leanring: $$Q(s, a)\\leftarrow (1-\\alpha)Q(s, a) + \\alpha[sample]$$\n",
    "where $sample = R(s, a, s') + \\gamma \\mathop{max}\\limits_{a'}Q_{k}(s', a')$ \n",
    "\n",
    "* Property\n",
    "\n",
    "    Result is amazing. Q-learning always converges to optimal policy even if you're acting suboptimally (off-policy learning)\n",
    "\n",
    "* Limitation\n",
    "\n",
    "    need to explore enough, and you have to eventually make the learning rate small enough (not decrease it quickly)\n",
    "\n",
    "**Exploration and Exploitation**\n",
    "\n",
    "* Exploration gives up a reward that you know about in order to learn more about the environment\n",
    "\n",
    "* Exploitation exploits known rewards to maximize the reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardenyu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52425358d21c1500e9f7e0b0b3202539c28bb8d46208ac7718471269cd3c6480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
